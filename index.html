---
layout: home
title: Decoding Biases Final 
subtitle: A presentation on How AI is affecting Trans individuals
---

<!DOCTYPE html>
<html lang="{{ page.language | default: site.language | default: 'en' }}">
<!-- Beautiful Jekyll 6.0.1 | Copyright Dean Attali 2023 -->
{% include head.html %}

<body>
  {% include gtm_body.html %}

  {% include nav.html %}
  {% include footer.html %}

  <h1>Abstract</h1>
  <p> This study investigates algorithmic bias in AI facial recognition systems, focusing on gender
    and age misclassification within the transgender community. Using a Caffe-based facial recognition model, 
    the research analyzes the performance of gender and age detection on a dataset of 207 individuals
    (105 transgender, 102 cisgender). The results reveal significant disparities: transgender women are especially 
    vulnerable to misgendering, with only 12 out of 79 correctly classified as female, in stark contrast to trans men,
     all of whom (26 out of 26) were correctly classified as male. Among cisgender individuals, the system accurately 
     identified 48 out of 49 men, but only 5 out of 53 women. Age estimation was also generally inaccurate. </p>
  <p>The model tended to predict lower average ages while exhibiting a broader distribution, often overestimating 
    the maximum age, particularly for men. These findings demonstrate that facial recognition algorithms misgender and 
    misage transgender individuals at a higher rate than cisgender individuals, with women, both cis and trans, most 
    frequently misclassified. The results align with previous research highlighting the exclusion of gender minorities 
    from training datasets and the inherent limitations of binary gender classification in commercial AI systems. This 
    study underscores the urgent need for more inclusive datasets and algorithmic refinements to mitigate bias,
    particularly against gender minorities and older adults.</p>
  
  {% include footer-scripts.html %}

</body>
</html>